# 深度学习评价指标

[文章来源](https://mp.weixin.qq.com/s/Ui6nCLOKKmwFYOITDYQ19A)

 深度学习模型的泛化性能通常根据评价指标对其进行衡量。在对比不同的模型的性能时，使用不同的指标往往会导致不同的评判结果。因此针对具体的数据、模型和任务需求应该选取与之对应的评价指标。本文主要介绍深度学习中分类及回归任务中常用的评价指标。

**1.** **回归任务中常用的评价指标**

(1) 平均绝对误差(Mean Absolute Error, MAE):

   平均绝对误差表示预测值和观测值之间绝对误差的平均值, 其数学表达式为：
$$
MAE=\frac{1}{n}\sum_{i=1}^n\vert\hat y_i - y_i\vert
$$
其中$y_i$表示真实值，$\hat y_i$表示预测值

```python
import numpy as np  
def mae_value(y_true, y_pred):  
""" 
参数: 
y_true -- 测试集目标真实值 
y_pred -- 测试集目标预测值 
返回: 
mae -- MAE 评价指标 
"""  
n = len(y_true)  
mae = sum(np.abs(y_true - y_pred))/n  
return mae
```

**(2) 平均绝对百分比误差(Mean Absolute Percentage Error, MAPE)**

   平均绝对百分比误差是一种相对度量，其将 MAE 尺度确定为百分比单位而不是变量的单位。其数学表达式为：
$$
MAPE=\frac{100\%}{n}\sum_{i=1}^n\vert\frac{\hat y_i-y_i}{y_i}\vert
$$
其中$y_i$表示真实值，$\hat y_i$表示预测值

```python
import numpy as np  
def mape(y_true, y_pred):  
""" 
参数: 
y_true -- 测试集目标真实值 
y_pred -- 测试集目标预测值 

返回: 
mape -- MAPE 评价指标 
"""  

n = len(y_true)  
mape = sum(np.abs((y_true - y_pred)/y_true))/n*100  
return mape 
```

**(3) 均方误差(Mean Square Error, MSE)**

均方误差是预测值和观测值差值的平方和的平均数，其数学表达式为：
$$
MSE=\frac{1}{n}\sum_{i=1}^n(\hat y_i-y_i)^2
$$
其中$y_i$表示真实值，$\hat y_i$表示预测值

```python
import numpy as np   
def mse_value(y_true, y_pred):  
""" 
参数: 
y_true -- 测试集目标真实值 
y_pred -- 测试集目标预测值 
 
返回: 
mse -- MSE 评价指标 
"""  
    
n = len(y_true)  
mse = sum(np.square(y_true - y_pred))/n  
return mse 
```

**(4) 均方根误差(Root Mean Square Error , RMSE)**

均方根误差是预测值与真实值偏差的平方与观测次数n比值的平方根. 其数学表达式为：
$$
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n(\hat y_i - y_i)^2}
$$
其中$y_i$表示真实值，$\hat y_i$表示预测值

```python
import numpy as np 
def rmse(y_true, y_pred):  
n = len(y_true)
res_rmse = np.sqrt(sum(np.square(y_true - y_pred))/n)  
return res_rmse
```

**2.** **语义分割任务中常用的评价指标**

**(1)****混淆矩阵**：

 混淆矩阵也称为误差矩阵，是表示精度评价的一种标准格式，其是以大小为$N\time N$的矩阵形式来表示。以二分类为例，其混淆矩阵如下：

| 真实情况\预测情况 | 正例       | 反例       |
| ----------------- | ---------- | ---------- |
| **正例**          | TP(真正例) | FN(假反例) |
| **反例**          | FP(假正例) | TN(真反例) |

**(2)总体分类精度（Overall Accuracy，简称ac）**

 分类正确的样本个数占所有样本个数的比例，其数学表达式如下：
$$
acc=\frac{TP+TN}{TP+FN+FP+TN}
$$
**(3)准确率(Precision)、召回率(Recall)、F1**

 准确率表示的是预测为正例的数据中预测正例的概率,召回率表示真实为正例的数据中预测正例的概率，其公式分别为
$$
Precision=\frac{TP}{TP+FP}\\
Recal=\frac{TP}{TP+FN}
$$
准确率和召回率是一对矛盾的度量。一般情况下，准确率高时，召回率通常较低，而召回率高，准确率通常较低。为综合考虑准确率和召回率两者的性能，提出F1度量，其是基于准确率和召回率的调和平均定义的，即F1满足
$$
\frac{1}{F1}=\frac{1}{2}(\frac{1}{Precision}+\frac{1}{Recall})
$$
可知F1的公式为
$$
F1=2\cdot\frac{Precision\cdot Recall}{Precision+Recall}
$$
其中$\beta>0$度量了召回率对准确率的相对重要性，$\beta=1$时退化为标准的F1； $\beta>1$时召回率有更多的影响；$\beta<1$ 时准确率有更大的影响。

**(4)ROC曲线和AUC**

 ROC全称为“受试者工作特征”(Receiver Operating Characteristic)曲线，其源于“二战”中用于敌机检测的雷达信号分析技术。根据预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算真正例率 (True Positive Rate, 简称TPR)和假正例率(False Positive Rate, 简称FPR)的值，并以FPR为横轴、TPR为纵轴作图，得到ROC曲线，其TPR、FPR的数学表达式为：
$$
TPR=\frac{TP}{TP+FN},FPR=\frac{FP}{TP+Fp}
$$
其中AUC表示ROC曲线下的面积，在此利用有限样例来展示ROC曲线与AUC，如下图所示：

![图片](https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/640.jpeg)

<center>（图片来自知乎，感谢分享）</center>

ROC绘制代码如下(代码来自CSDN, 感谢分享)：

```python
import numpy as np  
import matplotlib.pyplot as plt  
import pandas as pd  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold  
from sklearn.metrics import classification_report  
from sklearn import svm, datasets  
from sklearn.metrics import roc_curve, auc  
from scipy import interp  
def draw_auc():  
   
    X = np.array(pd_data.head(5000).drop(columns=["KILLED"]))  
    y = np.array(pd_data.head(5000)["KILLED"])  
    random_state = np.random.RandomState(0)  
   
    cv = StratifiedKFold(n_splits=6).split(X,y)  
    classifier = svm.SVC(kernel='linear', probability=True,  
    random_state=random_state)  # 注意这里，probability=True,需要，不然预测的时候会出现异常。另外rbf核效果更好些。  
   
    mean_tpr = 0.0  
    mean_fpr = np.linspace(0, 1, 100)  
    all_tpr = []  
   
    for i, (train, test) in enumerate(cv):  
        # 通过训练数据，使用svm线性核建立模型，并对测试集进行测试，求  出预测得分  
        probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])  
   
        # Compute ROC curve and area the curve  
        # 通过roc_curve()函数，求出fpr和tpr，以及阈值  
        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])  
        mean_tpr += interp(mean_fpr, fpr, tpr)  # 对mean_tpr在mean_fpr处进行插值，通过scipy包调用interp()函数  
        mean_tpr[0] = 0.0  # 初始处为0  
        roc_auc = auc(fpr, tpr)  
        # 画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数能计算出来  
        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))  
   
        # 画对角线  
    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')  
   
    mean_tpr /= len(list(cv))  # 在mean_fpr100个点，每个点处插值插值多次取平均  
    mean_tpr[-1] = 1.0  # 坐标最后一个点为（1,1）  
    mean_auc = auc(mean_fpr, mean_tpr)  # 计算平均AUC值  
    # 画平均ROC曲线  
    # print mean_fpr,len(mean_fpr)  
    # print mean_tpr  
    plt.plot(mean_fpr, mean_tpr, 'k--',  
             label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)  
   
    plt.xlim([-0.05, 1.05])  
    plt.ylim([-0.05, 1.05])  
    plt.xlabel('False Positive Rate')  
    plt.ylabel('True Positive Rate')  
    plt.title('Receiver operating characteristic example')  
    plt.legend(loc="lower right")  
    plt.show()  
```

**(5)IOU和mIOU (均以二分类为例)**

  Intersection overUnion(IOU：交并比): 对某一类别预测结果和真实值的交集与并集的比值，其公式为：
$$
\rm{IOU}=\frac{TP}{TP+FP+FN}
$$
Mean Intersection overUnion(mIOU，均交并比)：对**每一类**预测的结果和真实值的交集与并集的比值求平均的结果，其公式为:
$$
\rm{mIOU}=\frac{IOU_{正例}+IOU_{反例}}{2}=\frac{TP}{TP+FP+FN}+\frac{TN}{TN+FN+FP}
$$
**代码(Pytorch)：**

```python
from collections import OrderedDict  
import mmcv  
import numpy as np  
import torch  
  
#acc, precision, recall, IOU, mIOU的代码最终表现total_area_to_metrics   
#F1的代码   
def f_score(precision, recall, beta=1):  
    score = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)  
    return score  
   
def intersect_and_union(pred_label,  
                    label,  
                    num_classes,  
                    ignore_index,  
                    label_map=dict(),  
                    reduce_zero_label=False):  
      
    if isinstance(pred_label, str):  
        pred_label = torch.from_numpy(np.load(pred_label))  
    else:  
        pred_label = torch.from_numpy((pred_label))  
  
    if isinstance(label, str):  
        label = torch.from_numpy(  
            mmcv.imread(label, flag='unchanged', backend='pillow'))  
    else:  
        label = torch.from_numpy(label)  
  
    if label_map is not None:  
        for old_id, new_id in label_map.items():  
            label[label == old_id] = new_id  
    if reduce_zero_label:  
        label[label == 0] = 255  
        label = label - 1  
        label[label == 254] = 255  
  
    mask = (label != ignore_index)  
    pred_label = pred_label[mask]  
    label = label[mask]  
  
    intersect = pred_label[pred_label == label]  
    area_intersect = torch.histc(  
        intersect.float(), bins=(num_classes), min=0, max=num_classes - 1)  
    area_pred_label = torch.histc(  
        pred_label.float(), bins=(num_classes), min=0, max=num_classes - 1)  
    area_label = torch.histc(  
        label.float(), bins=(num_classes), min=0, max=num_classes - 1)  
    area_union = area_pred_label + area_label - area_intersect  
    return area_intersect, area_union, area_pred_label, area_label  
  
def total_intersect_and_union(results,  
                         gt_seg_maps,  
                         num_classes,  
                         ignore_index,  
                         label_map=dict(),  
                         reduce_zero_label=False):  
    
    num_imgs = len(results)  
    assert len(list(gt_seg_maps)) == num_imgs  
    total_area_intersect = torch.zeros((num_classes, ), dtype=torch.float64)  
    total_area_union = torch.zeros((num_classes, ), dtype=torch.float64)  
    total_area_pred_label = torch.zeros((num_classes, ), dtype=torch.float64)  
    total_area_label = torch.zeros((num_classes, ), dtype=torch.float64)  
    for result, gt_seg_map in zip(results, gt_seg_maps):  
        area_intersect, area_union, area_pred_label, area_label = \  
            intersect_and_union(  
                result, gt_seg_map, num_classes, ignore_index,  
                label_map, reduce_zero_label)  
        total_area_intersect += area_intersect  
        total_area_union += area_union  
        total_area_pred_label += area_pred_label  
        total_area_label += area_label  
    return total_area_intersect, total_area_union, total_area_pred_label, \  
        total_area_label  
  
def mean_iou(results,  
             gt_seg_maps,  
             num_classes,  
             ignore_index,  
             nan_to_num=None,  
             label_map=dict(),  
             reduce_zero_label=False):  
     
    iou_result = eval_metrics(  
        results=results,  
        gt_seg_maps=gt_seg_maps,  
        num_classes=num_classes,  
        ignore_index=ignore_index,  
        metrics=['mIoU'],  
        nan_to_num=nan_to_num,  
        label_map=label_map,  
        reduce_zero_label=reduce_zero_label)  
    return iou_result  
  
def mean_fscore(results,  
                gt_seg_maps,  
                num_classes,  
                ignore_index,  
                nan_to_num=None,  
                label_map=dict(),  
                reduce_zero_label=False,  
                beta=1):  
     
    fscore_result = eval_metrics(  
        results=results,  
        gt_seg_maps=gt_seg_maps,  
        num_classes=num_classes,  
        ignore_index=ignore_index,  
        metrics=['mFscore'],  
        nan_to_num=nan_to_num,  
        label_map=label_map,  
        reduce_zero_label=reduce_zero_label,  
        beta=beta)  
    return fscore_result  
  
def eval_metrics(results,  
              gt_seg_maps,  
              num_classes,  
              ignore_index,  
              metrics=['mIoU'],  
              nan_to_num=None,  
              label_map=dict(),  
              reduce_zero_label=False,  
              beta=1):  
  
    total_area_intersect, total_area_union, total_area_pred_label, \  
        total_area_label = total_intersect_and_union(  
            results, gt_seg_maps, num_classes, ignore_index, label_map,  
            reduce_zero_label)  
        ret_metrics = total_area_to_metrics(total_area_intersect, total_area_union,  
                                 total_area_pred_label,  
                                 total_area_label,metrics, nan_to_num,beta)  
    return ret_metrics  
  
  
def pre_eval_to_metrics(pre_eval_results,  
                        metrics=['mIoU'],  
                        nan_to_num=None,  
                        beta=1):  
      
    pre_eval_results = tuple(zip(*pre_eval_results))  
    assert len(pre_eval_results) == 4  
  
    total_area_intersect = sum(pre_eval_results[0])  
    total_area_union = sum(pre_eval_results[1])  
    total_area_pred_label = sum(pre_eval_results[2])  
    total_area_label = sum(pre_eval_results[3])  
  
    ret_metrics = total_area_to_metrics(total_area_intersect, total_area_union,  
                                 total_area_pred_label,  
                                 total_area_label, metrics, nan_to_num,  
                                 beta)  
    return ret_metrics  
  
  
def total_area_to_metrics(total_area_intersect,  
                     total_area_union,  
                     total_area_pred_label,  
                     total_area_label,  
                     metrics=['mIoU'],  
                     nan_to_num=None,  
                     beta=1):  
     
    if isinstance(metrics, str):  
        metrics = [metrics]  
    allowed_metrics = ['mIoU', 'mFscore']  
    if not set(metrics).issubset(set(allowed_metrics)):  
        raise KeyError('metrics {} is not supported'.format(metrics))  
  
    all_acc = total_area_intersect.sum() / total_area_label.sum()  
    ret_metrics = OrderedDict({'aAcc': all_acc})  
    for metric in metrics:  
        if metric == 'mIoU':  
            iou = total_area_intersect / total_area_union  
            acc = total_area_intersect / total_area_label  
            ret_metrics['IoU'] = iou  
            ret_metrics['Acc'] = acc  
        elif metric == 'mFscore':  
            precision = total_area_intersect / total_area_pred_label  
            recall = total_area_intersect / total_area_label  
            f_value = torch.tensor(  
                [f_score(x[0], x[1], beta) for x in zip(precision, recall)])  
            ret_metrics['Fscore'] = f_value  
            ret_metrics['Precision'] = precision  
            ret_metrics['Recall'] = recall  
  
    ret_metrics = {  
        metric: value.numpy()  
        for metric, value in ret_metrics.items()  
    }  
    if nan_to_num is not None:  
        ret_metrics = OrderedDict({  
            metric: np.nan_to_num(metric_value, nan=nan_to_num)  
            for metric, metric_value in ret_metrics.items()  
        })  
    return ret_metrics  
```

